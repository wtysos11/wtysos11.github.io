<!DOCTYPE html>
<html lang="zh">

<head>
  <title>
  Conv1d原理解析 · 实践出真知
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Timothy Wu">
<meta name="description" content="torch中的Conv1d解析">
<meta name="keywords" content="blog,developer,personal">

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Conv1d原理解析"/>
<meta name="twitter:description" content="torch中的Conv1d解析"/>

<meta property="og:title" content="Conv1d原理解析" />
<meta property="og:description" content="torch中的Conv1d解析" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://wtysos11.github.io/posts/20210927_conv1d%E5%87%BD%E6%95%B0%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-09-27T09:49:02+08:00" />
<meta property="article:modified_time" content="2021-09-27T09:49:02+08:00" />





<link rel="canonical" href="http://wtysos11.github.io/posts/20210927_conv1d%E5%87%BD%E6%95%B0%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.577e3c5ead537873430da16f0964b754a120fd87c4e2203a00686e7c75b51378.css" integrity="sha256-V348Xq1TeHNDDaFvCWS3VKEg/YfE4iA6AGhufHW1E3g=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css" integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin="anonymous" media="screen" />
  



 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      实践出真知
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/tags/">Tags</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/categories/">Categories</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/series/">series</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://wtysos11.github.io/posts/20210927_conv1d%E5%87%BD%E6%95%B0%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/">
              Conv1d原理解析
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2021-09-27T09:49:02&#43;08:00">
                九月 27, 2021
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              
            </span>
          </div>
          
          <div class="categories">
  <i class="fa-solid fa-folder" aria-hidden="true"></i>
    <a href="/categories/%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">开源代码学习笔记</a></div>

          <div class="tags">
  <i class="fa-solid fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/python/">计算机/python</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/%E5%86%85%E5%AE%B9/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">内容/学习笔记</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/pytorch/">计算机/pytorch</a>
    </span></div>

        </div>
      </header>

      <div class="post-content">
        
        <blockquote>
<p>今天碰上了需要使用Conv1d的场景，但是对于in_channel，out_channel和kernel_size所影响的Conv1d层而进行的操作还是十分的迷惑，因此写下此篇文章记录自己的学习过程。</p>
</blockquote>
<h2 id="公式">
  公式
  <a class="heading-link" href="#%e5%85%ac%e5%bc%8f">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html"  class="external-link" target="_blank" rel="noopener">官方文档</a></p>
<p>![formula](/assets/20210927 conv1d/Conv1d_formula.png)</p>
<p>从公式可以看出，输入到Conv1d中的数据有三个维度，第一个维度N一般是batch_size，第二个维度一般为in_channel，第三个维度为序列的时间维度，在NLP中为词向量大小；输出维度基本相同，但是输出的第二个维度为out_channel。</p>
<p>公式限定了第i个bathc_size中输出的第j个channel。在计算过程中，bias自然不必多讲，求和内的k指遍历所有的in_channel，然后使用对应的权重和指定的输入向量进行卷积操作。</p>
<h2 id="计算例子">
  计算例子
  <a class="heading-link" href="#%e8%ae%a1%e7%ae%97%e4%be%8b%e5%ad%90">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>如果懂的人已经可以看懂这条公式了，可是我不懂……所以还是用例子来说明一下</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">import</span> <span style="color:#ff7b72">torch</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">import</span> <span style="color:#ff7b72">torch.nn</span> <span style="color:#ff7b72">as</span> <span style="color:#ff7b72">nn</span>
</span></span><span style="display:flex;"><span>test_layer <span style="color:#ff7b72;font-weight:bold">=</span> nn<span style="color:#ff7b72;font-weight:bold">.</span>Conv1d(in_channels<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">3</span>, out_channels<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">2</span>, kernel_size<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">4</span>)  <span style="color:#8b949e;font-style:italic"># 设计一个测试层，不同数据不一样，方便后面查阅</span>
</span></span><span style="display:flex;"><span>print(test_layer<span style="color:#ff7b72;font-weight:bold">.</span>weight<span style="color:#ff7b72;font-weight:bold">.</span>shape)  <span style="color:#8b949e;font-style:italic">#  [2,3,4]，即公式中的weight。对于每一个out_channel和in_channel的对应，都有一个kernel_size大小的卷积核</span>
</span></span><span style="display:flex;"><span>test_data <span style="color:#ff7b72;font-weight:bold">=</span> torch<span style="color:#ff7b72;font-weight:bold">.</span>rand(<span style="color:#a5d6ff">1</span>,<span style="color:#a5d6ff">3</span>,<span style="color:#a5d6ff">10</span>) <span style="color:#8b949e;font-style:italic"># 输入测试数据，3个channel，时间维为10</span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># 概要测试</span>
</span></span><span style="display:flex;"><span>output <span style="color:#ff7b72;font-weight:bold">=</span> test_layer(test_data) 
</span></span><span style="display:flex;"><span>print(output<span style="color:#ff7b72;font-weight:bold">.</span>shape)  <span style="color:#8b949e;font-style:italic"># [1,2,7] 2为out_channel，7为L_out，具体计算公式可参见官方文档</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># 具体计算，以out(0, 0, 0)为例，即Ni=0, Coutj=0的第一个元素</span>
</span></span><span style="display:flex;"><span>print(output[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">0</span>])  <span style="color:#8b949e;font-style:italic"># [0.2545, 0.3342, 0.3826, 0.1345, 0.0378, 0.2512, 0.2467]</span>
</span></span><span style="display:flex;"><span>print(test_data[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">0</span>],test_data[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">1</span>],test_data[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">2</span>])
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># tensor([0.4535, 0.6660, 0.1077, 0.7335, 0.8431, 0.2407, 0.2267, 0.1635, 0.8010, 0.5360]) </span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># tensor([0.5334, 0.7020, 0.7540, 0.7194, 0.9105, 0.2495, 0.3046, 0.3894, 0.6813, 0.0660]) </span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># tensor([0.5396, 0.6200, 0.7067, 0.9654, 0.8220, 0.8894, 0.5200, 0.9175, 0.6874, 0.8831])</span>
</span></span><span style="display:flex;"><span>print(test_layer<span style="color:#ff7b72;font-weight:bold">.</span>weight[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">0</span>], test_layer<span style="color:#ff7b72;font-weight:bold">.</span>weight[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">1</span>],test_layer<span style="color:#ff7b72;font-weight:bold">.</span>weight[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">2</span>])
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># tensor([-0.1468, -0.0057,  0.0926,  0.0263], grad_fn=&lt;SelectBackward&gt;) </span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># tensor([ 0.1042,  0.0158,  0.2408, -0.0116], grad_fn=&lt;SelectBackward&gt;) </span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># tensor([ 0.0223, -0.2104,  0.1971, -0.0318], grad_fn=&lt;SelectBackward&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic">## 第0个元素的计算，即为所有的in_channel与weight上对应的kernel_size做卷积的结果</span>
</span></span><span style="display:flex;"><span>a1 <span style="color:#ff7b72;font-weight:bold">=</span> torch<span style="color:#ff7b72;font-weight:bold">.</span>sum(test_layer<span style="color:#ff7b72;font-weight:bold">.</span>weight[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">0</span>]<span style="color:#ff7b72;font-weight:bold">*</span>test_data[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">0</span>:<span style="color:#a5d6ff">4</span>])
</span></span><span style="display:flex;"><span>a2 <span style="color:#ff7b72;font-weight:bold">=</span> torch<span style="color:#ff7b72;font-weight:bold">.</span>sum(test_layer<span style="color:#ff7b72;font-weight:bold">.</span>weight[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">1</span>]<span style="color:#ff7b72;font-weight:bold">*</span>test_data[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">1</span>,<span style="color:#a5d6ff">0</span>:<span style="color:#a5d6ff">4</span>])
</span></span><span style="display:flex;"><span>a3 <span style="color:#ff7b72;font-weight:bold">=</span> torch<span style="color:#ff7b72;font-weight:bold">.</span>sum(test_layer<span style="color:#ff7b72;font-weight:bold">.</span>weight[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">2</span>]<span style="color:#ff7b72;font-weight:bold">*</span>test_data[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">2</span>,<span style="color:#a5d6ff">0</span>:<span style="color:#a5d6ff">4</span>])
</span></span><span style="display:flex;"><span>print(a1<span style="color:#ff7b72;font-weight:bold">+</span>a2<span style="color:#ff7b72;font-weight:bold">+</span>a3) <span style="color:#8b949e;font-style:italic"># tensor(0.1890, grad_fn=&lt;AddBackward0&gt;)</span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic">## 但是第0个元素 output[0,0,0] = 0.2545，不对啊……这是因为少了bias。加上bias[0]就对了</span>
</span></span><span style="display:flex;"><span>print(test_layer<span style="color:#ff7b72;font-weight:bold">.</span>bias) <span style="color:#8b949e;font-style:italic"># tensor([ 0.0655, -0.1095], requires_grad=True)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># 另外一个例子，output(0, 1, 2)，即out_channel=1的第二个元素，值为0.3290</span>
</span></span><span style="display:flex;"><span>print(output[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">1</span>])
</span></span><span style="display:flex;"><span>print(test_data[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">0</span>],test_data[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">1</span>],test_data[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">2</span>])
</span></span><span style="display:flex;"><span>print(test_layer<span style="color:#ff7b72;font-weight:bold">.</span>weight[<span style="color:#a5d6ff">1</span>,<span style="color:#a5d6ff">0</span>], test_layer<span style="color:#ff7b72;font-weight:bold">.</span>weight[<span style="color:#a5d6ff">1</span>,<span style="color:#a5d6ff">1</span>],test_layer<span style="color:#ff7b72;font-weight:bold">.</span>weight[<span style="color:#a5d6ff">1</span>,<span style="color:#a5d6ff">2</span>])
</span></span><span style="display:flex;"><span>a1 <span style="color:#ff7b72;font-weight:bold">=</span> torch<span style="color:#ff7b72;font-weight:bold">.</span>sum(test_layer<span style="color:#ff7b72;font-weight:bold">.</span>weight[<span style="color:#a5d6ff">1</span>,<span style="color:#a5d6ff">0</span>]<span style="color:#ff7b72;font-weight:bold">*</span>test_data[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">2</span>:<span style="color:#a5d6ff">6</span>])
</span></span><span style="display:flex;"><span>a2 <span style="color:#ff7b72;font-weight:bold">=</span> torch<span style="color:#ff7b72;font-weight:bold">.</span>sum(test_layer<span style="color:#ff7b72;font-weight:bold">.</span>weight[<span style="color:#a5d6ff">1</span>,<span style="color:#a5d6ff">1</span>]<span style="color:#ff7b72;font-weight:bold">*</span>test_data[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">1</span>,<span style="color:#a5d6ff">2</span>:<span style="color:#a5d6ff">6</span>])
</span></span><span style="display:flex;"><span>a3 <span style="color:#ff7b72;font-weight:bold">=</span> torch<span style="color:#ff7b72;font-weight:bold">.</span>sum(test_layer<span style="color:#ff7b72;font-weight:bold">.</span>weight[<span style="color:#a5d6ff">1</span>,<span style="color:#a5d6ff">2</span>]<span style="color:#ff7b72;font-weight:bold">*</span>test_data[<span style="color:#a5d6ff">0</span>,<span style="color:#a5d6ff">2</span>,<span style="color:#a5d6ff">2</span>:<span style="color:#a5d6ff">6</span>])
</span></span><span style="display:flex;"><span>print(a1<span style="color:#ff7b72;font-weight:bold">+</span>a2<span style="color:#ff7b72;font-weight:bold">+</span>a3)  <span style="color:#8b949e;font-style:italic"># 0.4385，减去bias[1](-0.1095)，结果正确</span>
</span></span></code></pre></div>
      </div>


      <footer>
        


        
        
        
        
        

        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2020 -
    
    2024
     Timothy Wu 
    ·
    
     <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js" integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>

</html>
