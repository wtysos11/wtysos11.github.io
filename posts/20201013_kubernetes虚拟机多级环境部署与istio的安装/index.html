<!DOCTYPE html>
<html lang="zh">

<head>
  <title>
  kubernetes虚拟机多级环境部署与Istio的安装 · 实践出真知
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Timothy Wu">
<meta name="description" content="后续更新，不一定会记得更新
kubernetes虚拟机多级环境部署与Istio的安装 Link to heading 标签：项目实践 kubernetes istio
计划在三台2C4G的机器上安装kubernetes。其中master和slave的系统均为Ubuntu18.04桌面版。（原本想用CentOS的，下载的时候就是这样了，费事改了）
目前需要在实验室的服务器上进行部署，得了，最终结果也是一样的。
分为以下几个步骤：
安装基本软件 访问谷歌镜像仓库gcr.io 完成三台机器关于安装kubeadm的相关工作 安装Istio 部署相关应用（prometheus、grafana） 参考文献：
掘金 cnblog，这一篇是我后来看到的，写的更好一些。 安装基本软件 Link to heading 如果使用包管理软件，请务必检查自身的包管理软件为最新。
以Ubuntu的apt为例，首先请务必使用最新的镜像仓库（最好手动弄一下），然后再执行一遍sudo apt-get update来更新。
配置docker Link to heading 略
配置kubectl、kubeadm、kubelet Link to heading 在执行命令之前，请首先使用检查是否能够访问到指定的包。以apt为例，使用apt-cache policy &lt;package&gt;可以检查远程仓库中包的版本。
apt-get install kubectl kubeadm kubelet 系统配置 Link to heading 关闭防火墙 关闭SELinux 关闭swap 下载kubeadm所需镜像 Link to heading 从阿里云下载 Link to heading 首先使用kubeadm config images list列出kubeadm所需要的所有镜像
k8s.gcr.io/kube-apiserver:v1.19.2 k8s.gcr.io/kube-controller-manager:v1.19.2 k8s.gcr.io/kube-scheduler:v1.19.2 k8s.gcr.io/kube-proxy:v1.19.2 k8s.gcr.io/pause:3.2 k8s.gcr.io/etcd:3.4.13-0 k8s.gcr.io/coredns:1.7.0 即所有的系统镜像为v1.">
<meta name="keywords" content="blog,developer,personal">

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="kubernetes虚拟机多级环境部署与Istio的安装"/>
<meta name="twitter:description" content="后续更新，不一定会记得更新
kubernetes虚拟机多级环境部署与Istio的安装 Link to heading 标签：项目实践 kubernetes istio
计划在三台2C4G的机器上安装kubernetes。其中master和slave的系统均为Ubuntu18.04桌面版。（原本想用CentOS的，下载的时候就是这样了，费事改了）
目前需要在实验室的服务器上进行部署，得了，最终结果也是一样的。
分为以下几个步骤：
安装基本软件 访问谷歌镜像仓库gcr.io 完成三台机器关于安装kubeadm的相关工作 安装Istio 部署相关应用（prometheus、grafana） 参考文献：
掘金 cnblog，这一篇是我后来看到的，写的更好一些。 安装基本软件 Link to heading 如果使用包管理软件，请务必检查自身的包管理软件为最新。
以Ubuntu的apt为例，首先请务必使用最新的镜像仓库（最好手动弄一下），然后再执行一遍sudo apt-get update来更新。
配置docker Link to heading 略
配置kubectl、kubeadm、kubelet Link to heading 在执行命令之前，请首先使用检查是否能够访问到指定的包。以apt为例，使用apt-cache policy &lt;package&gt;可以检查远程仓库中包的版本。
apt-get install kubectl kubeadm kubelet 系统配置 Link to heading 关闭防火墙 关闭SELinux 关闭swap 下载kubeadm所需镜像 Link to heading 从阿里云下载 Link to heading 首先使用kubeadm config images list列出kubeadm所需要的所有镜像
k8s.gcr.io/kube-apiserver:v1.19.2 k8s.gcr.io/kube-controller-manager:v1.19.2 k8s.gcr.io/kube-scheduler:v1.19.2 k8s.gcr.io/kube-proxy:v1.19.2 k8s.gcr.io/pause:3.2 k8s.gcr.io/etcd:3.4.13-0 k8s.gcr.io/coredns:1.7.0 即所有的系统镜像为v1."/>

<meta property="og:title" content="kubernetes虚拟机多级环境部署与Istio的安装" />
<meta property="og:description" content="后续更新，不一定会记得更新
kubernetes虚拟机多级环境部署与Istio的安装 Link to heading 标签：项目实践 kubernetes istio
计划在三台2C4G的机器上安装kubernetes。其中master和slave的系统均为Ubuntu18.04桌面版。（原本想用CentOS的，下载的时候就是这样了，费事改了）
目前需要在实验室的服务器上进行部署，得了，最终结果也是一样的。
分为以下几个步骤：
安装基本软件 访问谷歌镜像仓库gcr.io 完成三台机器关于安装kubeadm的相关工作 安装Istio 部署相关应用（prometheus、grafana） 参考文献：
掘金 cnblog，这一篇是我后来看到的，写的更好一些。 安装基本软件 Link to heading 如果使用包管理软件，请务必检查自身的包管理软件为最新。
以Ubuntu的apt为例，首先请务必使用最新的镜像仓库（最好手动弄一下），然后再执行一遍sudo apt-get update来更新。
配置docker Link to heading 略
配置kubectl、kubeadm、kubelet Link to heading 在执行命令之前，请首先使用检查是否能够访问到指定的包。以apt为例，使用apt-cache policy &lt;package&gt;可以检查远程仓库中包的版本。
apt-get install kubectl kubeadm kubelet 系统配置 Link to heading 关闭防火墙 关闭SELinux 关闭swap 下载kubeadm所需镜像 Link to heading 从阿里云下载 Link to heading 首先使用kubeadm config images list列出kubeadm所需要的所有镜像
k8s.gcr.io/kube-apiserver:v1.19.2 k8s.gcr.io/kube-controller-manager:v1.19.2 k8s.gcr.io/kube-scheduler:v1.19.2 k8s.gcr.io/kube-proxy:v1.19.2 k8s.gcr.io/pause:3.2 k8s.gcr.io/etcd:3.4.13-0 k8s.gcr.io/coredns:1.7.0 即所有的系统镜像为v1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://wtysos11.github.io/posts/20201013_kubernetes%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%A4%9A%E7%BA%A7%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E4%B8%8Eistio%E7%9A%84%E5%AE%89%E8%A3%85/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-10-13T12:37:13+08:00" />
<meta property="article:modified_time" content="2020-10-13T12:37:13+08:00" />





<link rel="canonical" href="http://wtysos11.github.io/posts/20201013_kubernetes%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%A4%9A%E7%BA%A7%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E4%B8%8Eistio%E7%9A%84%E5%AE%89%E8%A3%85/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.577e3c5ead537873430da16f0964b754a120fd87c4e2203a00686e7c75b51378.css" integrity="sha256-V348Xq1TeHNDDaFvCWS3VKEg/YfE4iA6AGhufHW1E3g=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css" integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin="anonymous" media="screen" />
  



 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      实践出真知
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/tags/">Tags</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/categories/">Categories</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/series/">series</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://wtysos11.github.io/posts/20201013_kubernetes%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%A4%9A%E7%BA%A7%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E4%B8%8Eistio%E7%9A%84%E5%AE%89%E8%A3%85/">
              kubernetes虚拟机多级环境部署与Istio的安装
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2020-10-13T12:37:13&#43;08:00">
                十月 13, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              
            </span>
          </div>
          
          <div class="categories">
  <i class="fa-solid fa-folder" aria-hidden="true"></i>
    <a href="/categories/%E6%93%8D%E4%BD%9C%E5%AE%9E%E8%B7%B5/">操作实践</a></div>

          <div class="tags">
  <i class="fa-solid fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/%E5%86%85%E5%AE%B9/%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/">内容/实践记录</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/kubernetes/istio/">计算机/kubernetes/istio</a>
    </span></div>

        </div>
      </header>

      <div class="post-content">
        
        <p><a href="https://github.com/wtysos11/blogWiki/blob/master/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5/k8s/kubernetes%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%A4%9A%E6%9C%BA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2.md"  class="external-link" target="_blank" rel="noopener">后续更新</a>，不一定会记得更新</p>
<h1 id="kubernetes虚拟机多级环境部署与istio的安装">
  kubernetes虚拟机多级环境部署与Istio的安装
  <a class="heading-link" href="#kubernetes%e8%99%9a%e6%8b%9f%e6%9c%ba%e5%a4%9a%e7%ba%a7%e7%8e%af%e5%a2%83%e9%83%a8%e7%bd%b2%e4%b8%8eistio%e7%9a%84%e5%ae%89%e8%a3%85">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<p>标签：项目实践 kubernetes istio</p>
<p>计划在三台2C4G的机器上安装kubernetes。其中master和slave的系统均为Ubuntu18.04桌面版。（原本想用CentOS的，下载的时候就是这样了，费事改了）</p>
<p>目前需要在实验室的服务器上进行部署，得了，最终结果也是一样的。</p>
<p>分为以下几个步骤：</p>
<ol>
<li>安装基本软件</li>
<li>访问谷歌镜像仓库gcr.io</li>
<li>完成三台机器关于安装kubeadm的相关工作</li>
<li>安装Istio</li>
<li>部署相关应用（prometheus、grafana）</li>
</ol>
<p>参考文献：</p>
<ul>
<li><a href="https://juejin.im/post/6844903908326932493"  class="external-link" target="_blank" rel="noopener">掘金</a></li>
<li><a href="https://www.cnblogs.com/tylerzhou/p/10971336.html"  class="external-link" target="_blank" rel="noopener">cnblog</a>，这一篇是我后来看到的，写的更好一些。</li>
</ul>
<h2 id="安装基本软件">
  安装基本软件
  <a class="heading-link" href="#%e5%ae%89%e8%a3%85%e5%9f%ba%e6%9c%ac%e8%bd%af%e4%bb%b6">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>如果使用包管理软件，请务必检查自身的包管理软件为最新。</p>
<p>以Ubuntu的apt为例，首先请务必使用最新的镜像仓库（最好手动弄一下），然后再执行一遍<code>sudo apt-get update</code>来更新。</p>
<h3 id="配置docker">
  配置docker
  <a class="heading-link" href="#%e9%85%8d%e7%bd%aedocker">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>略</p>
<h3 id="配置kubectlkubeadmkubelet">
  配置kubectl、kubeadm、kubelet
  <a class="heading-link" href="#%e9%85%8d%e7%bd%aekubectlkubeadmkubelet">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>在执行命令之前，请首先使用检查是否能够访问到指定的包。以apt为例，使用<code>apt-cache policy &lt;package&gt;</code>可以检查远程仓库中包的版本。</p>
<ul>
<li><code>apt-get install kubectl kubeadm kubelet</code></li>
</ul>
<h3 id="系统配置">
  系统配置
  <a class="heading-link" href="#%e7%b3%bb%e7%bb%9f%e9%85%8d%e7%bd%ae">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ul>
<li>关闭防火墙</li>
<li>关闭SELinux</li>
<li>关闭swap</li>
</ul>
<h2 id="下载kubeadm所需镜像">
  下载kubeadm所需镜像
  <a class="heading-link" href="#%e4%b8%8b%e8%bd%bdkubeadm%e6%89%80%e9%9c%80%e9%95%9c%e5%83%8f">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<h3 id="从阿里云下载">
  从阿里云下载
  <a class="heading-link" href="#%e4%bb%8e%e9%98%bf%e9%87%8c%e4%ba%91%e4%b8%8b%e8%bd%bd">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>首先使用<code>kubeadm config images list</code>列出kubeadm所需要的所有镜像</p>
<pre tabindex="0"><code>k8s.gcr.io/kube-apiserver:v1.19.2
k8s.gcr.io/kube-controller-manager:v1.19.2
k8s.gcr.io/kube-scheduler:v1.19.2
k8s.gcr.io/kube-proxy:v1.19.2
k8s.gcr.io/pause:3.2
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/coredns:1.7.0
</code></pre><p>即所有的系统镜像为v1.19.2版本，pause3.2,etcd3.4.13,coredns1.7.0</p>
<p><a href="https://juejin.im/post/6844903908326932493"  class="external-link" target="_blank" rel="noopener">这篇博客</a>提供了一个脚本</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic">#########################################################################</span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># File Name: pull_master_image.sh</span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># Description: pull_master_image.sh</span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># Author: zhangyi</span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># mail: 450575982@qq.com</span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># Created Time: 2019-07-31 21:38:14</span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic">#########################################################################</span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic">#!/bin/bash</span>
</span></span><span style="display:flex;"><span><span style="color:#79c0ff">kube_version</span><span style="color:#ff7b72;font-weight:bold">=</span>:v1.19.2
</span></span><span style="display:flex;"><span><span style="color:#79c0ff">kube_images</span><span style="color:#ff7b72;font-weight:bold">=(</span>kube-proxy kube-scheduler kube-controller-manager kube-apiserver<span style="color:#ff7b72;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span><span style="color:#79c0ff">addon_images</span><span style="color:#ff7b72;font-weight:bold">=(</span>etcd:3.4.13-0 coredns:1.7.0 pause:3.2<span style="color:#ff7b72;font-weight:bold">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">for</span> imageName in <span style="color:#a5d6ff">${</span><span style="color:#79c0ff">kube_images</span>[@]<span style="color:#a5d6ff">}</span> ; <span style="color:#ff7b72">do</span>
</span></span><span style="display:flex;"><span>  docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/<span style="color:#79c0ff">$imageName</span>-amd64<span style="color:#79c0ff">$kube_version</span>
</span></span><span style="display:flex;"><span>  docker image tag registry.cn-hangzhou.aliyuncs.com/google_containers/<span style="color:#79c0ff">$imageName</span>-amd64<span style="color:#79c0ff">$kube_version</span> k8s.gcr.io/<span style="color:#79c0ff">$imageName$kube_version</span>
</span></span><span style="display:flex;"><span>  docker image rm registry.cn-hangzhou.aliyuncs.com/google_containers/<span style="color:#79c0ff">$imageName</span>-amd64<span style="color:#79c0ff">$kube_version</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">done</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">for</span> imageName in <span style="color:#a5d6ff">${</span><span style="color:#79c0ff">addon_images</span>[@]<span style="color:#a5d6ff">}</span> ; <span style="color:#ff7b72">do</span>
</span></span><span style="display:flex;"><span>  docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/<span style="color:#79c0ff">$imageName</span>
</span></span><span style="display:flex;"><span>  docker image tag registry.cn-hangzhou.aliyuncs.com/google_containers/<span style="color:#79c0ff">$imageName</span> k8s.gcr.io/<span style="color:#79c0ff">$imageName</span>
</span></span><span style="display:flex;"><span>  docker image rm registry.cn-hangzhou.aliyuncs.com/google_containers/<span style="color:#79c0ff">$imageName</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">done</span>
</span></span></code></pre></div><p>这个脚本应该是没有问题的，有可能因为阿里云那边的容器镜像发生改变导致脚本需要变化，但大体思路是没错的。</p>
<p>可能的错误信息：</p>
<ul>
<li>如果使用<code>sh</code>命令运行此脚本，可能会遇到<code>pullImage.sh: 3: pullImage.sh: Syntax error: &quot;(&quot; unexpected</code>。参考<a href="https://unix.stackexchange.com/questions/253892/syntax-error-unexpected-when-creating-an-array"  class="external-link" target="_blank" rel="noopener">这篇回答</a>，该问题的原因是因为使用<code>sh xx.sh</code>的时候是使用sh shell，是没有数组的。正确的执行方式是<code>bash xx.sh</code>或者如回答中一样使用<code>./xx.sh</code>(需要注意权限)</li>
</ul>
<h2 id="使用kubeadm安装kubernetes集群">
  使用kubeadm安装kubernetes集群
  <a class="heading-link" href="#%e4%bd%bf%e7%94%a8kubeadm%e5%ae%89%e8%a3%85kubernetes%e9%9b%86%e7%be%a4">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<h3 id="安装集群主体">
  安装集群主体
  <a class="heading-link" href="#%e5%ae%89%e8%a3%85%e9%9b%86%e7%be%a4%e4%b8%bb%e4%bd%93">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>在执行完第一步和第二步之后，可以直接使用kubeadm进行安装：<code>sudo kubeadm init --apiserver-advertise-address=192.168.1.1 --pod-network-cidr=10.244.0.0/16 --token-ttl=0</code>（与之相反，执行<code>kubeadm reset</code>就可以尽量还原<code>kubeadm init</code>或者<code>kubeadm join</code>所带来的影响）</p>
<p>命令参考：<a href="https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/"  class="external-link" target="_blank" rel="noopener">官方</a>，也可以在命令行下查看。</p>
<ul>
<li>apiserver-advertise-address：这个参数指定了监听的API地址。若没有设置，则使用默认网络接口。</li>
<li>apiserver-bind-port：这个参数指定了API服务器暴露出的端口号，默认是6443。</li>
<li>pod-network-cidr：规定了pod能够使用的IP地址段。我之前用的是16位子网掩码，但是现在给的子网就是24位掩码，我也不确定使用其他子网能不能行……先保险起见吧。</li>
<li>kubernetes-version：指定kubeadm安装的kubernetes版本。这个是很重要的，因为默认情况下kubeadm会安装与它版本相同的kubernetes版本，而由于国内的网络问题，每次都需要重新下载一遍镜像，非常的麻烦。如果之后版本使用这个脚本，可以加上<code>--kubernetes-version=v1.19.2</code></li>
<li>image-repository：默认是&quot;k8s.gcr.io&quot;。我觉得如果修改这个可以不用像之前那样从阿里云下载下来后手动tag，不过没有尝试。</li>
<li>token-ttl：令牌被删除前的时间，默认是24h。kubeadm初始化完毕后会生成一个令牌，让其他节点能够加入集群，过时之后这个令牌会自动删除。如果设置为0之后令牌就永不过期。</li>
</ul>
<p>这一步的难点在于如何设置pod-network-cidr，参数的<a href="https://blog.csdn.net/shida_csdn/article/details/104334372"  class="external-link" target="_blank" rel="noopener">作用</a>。根据<a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/"  class="external-link" target="_blank" rel="noopener">官方教程</a>的说法，Pod网络与任何主机网络不得有重叠。但是目前我看到的很多教程都是在主机局域网络下构建的。比如说三台主机都在<code>192.168.1.1/16</code>子网，而pod网络也在同样的子网下。</p>
<p>这个参数的设置似乎与所使用的CNI有关系：</p>
<ul>
<li><a href="https://coreos.com/flannel/docs/latest/kubernetes.html"  class="external-link" target="_blank" rel="noopener">flannel</a>，要求的参数为<code>--pod-network-cidr=10.244.0.0/16</code></li>
<li><a href="https://docs.projectcalico.org/getting-started/kubernetes/quickstart"  class="external-link" target="_blank" rel="noopener">calico</a>，要求的参数为<code>--pod-network-cidr=192.168.0.0/16</code></li>
</ul>
<p>本文采用flannel，一个很重要的原因是因为服务器的子网与pod的子网部分重叠，可能存在风险，以及flannel似乎更容易部署一些。</p>
<p>产生的日志：</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>stack@workflow-1:~/k8sTest$ sudo kubeadm init --apiserver-advertise-address<span style="color:#ff7b72;font-weight:bold">=</span>192.168.1.1 --pod-network-cidr<span style="color:#ff7b72;font-weight:bold">=</span>10.244.0.0/16 --token-ttl<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">0</span>
</span></span><span style="display:flex;"><span>W1012 19:18:16.455833   <span style="color:#a5d6ff">86405</span> configset.go:348<span style="color:#ff7b72;font-weight:bold">]</span> WARNING: kubeadm cannot validate component configs <span style="color:#ff7b72">for</span> API groups <span style="color:#ff7b72;font-weight:bold">[</span>kubelet.config.k8s.io kubeproxy.config.k8s.io<span style="color:#ff7b72;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>init<span style="color:#ff7b72;font-weight:bold">]</span> Using Kubernetes version: v1.19.2
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>preflight<span style="color:#ff7b72;font-weight:bold">]</span> Running pre-flight checks
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>preflight<span style="color:#ff7b72;font-weight:bold">]</span> Pulling images required <span style="color:#ff7b72">for</span> setting up a Kubernetes cluster
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>preflight<span style="color:#ff7b72;font-weight:bold">]</span> This might take a minute or two, depending on the speed of your internet connection
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>preflight<span style="color:#ff7b72;font-weight:bold">]</span> You can also perform this action in beforehand using <span style="color:#a5d6ff">&#39;kubeadm config images pull&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>certs<span style="color:#ff7b72;font-weight:bold">]</span> Using certificateDir folder <span style="color:#a5d6ff">&#34;/etc/kubernetes/pki&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>certs<span style="color:#ff7b72;font-weight:bold">]</span> Generating <span style="color:#a5d6ff">&#34;ca&#34;</span> certificate and key
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>certs<span style="color:#ff7b72;font-weight:bold">]</span> Generating <span style="color:#a5d6ff">&#34;apiserver&#34;</span> certificate and key
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>certs<span style="color:#ff7b72;font-weight:bold">]</span> apiserver serving cert is signed <span style="color:#ff7b72">for</span> DNS names <span style="color:#ff7b72;font-weight:bold">[</span>kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local workflow-1<span style="color:#ff7b72;font-weight:bold">]</span> and IPs <span style="color:#ff7b72;font-weight:bold">[</span>10.96.0.1 192.168.1.1<span style="color:#ff7b72;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>certs<span style="color:#ff7b72;font-weight:bold">]</span> Generating <span style="color:#a5d6ff">&#34;apiserver-kubelet-client&#34;</span> certificate and key
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>certs<span style="color:#ff7b72;font-weight:bold">]</span> Generating <span style="color:#a5d6ff">&#34;front-proxy-ca&#34;</span> certificate and key
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>certs<span style="color:#ff7b72;font-weight:bold">]</span> Generating <span style="color:#a5d6ff">&#34;front-proxy-client&#34;</span> certificate and key
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>certs<span style="color:#ff7b72;font-weight:bold">]</span> Generating <span style="color:#a5d6ff">&#34;etcd/ca&#34;</span> certificate and key
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>certs<span style="color:#ff7b72;font-weight:bold">]</span> Generating <span style="color:#a5d6ff">&#34;etcd/server&#34;</span> certificate and key
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>certs<span style="color:#ff7b72;font-weight:bold">]</span> etcd/server serving cert is signed <span style="color:#ff7b72">for</span> DNS names <span style="color:#ff7b72;font-weight:bold">[</span>localhost workflow-1<span style="color:#ff7b72;font-weight:bold">]</span> and IPs <span style="color:#ff7b72;font-weight:bold">[</span>192.168.1.1 127.0.0.1 ::1<span style="color:#ff7b72;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>certs<span style="color:#ff7b72;font-weight:bold">]</span> Generating <span style="color:#a5d6ff">&#34;etcd/peer&#34;</span> certificate and key
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>certs<span style="color:#ff7b72;font-weight:bold">]</span> etcd/peer serving cert is signed <span style="color:#ff7b72">for</span> DNS names <span style="color:#ff7b72;font-weight:bold">[</span>localhost workflow-1<span style="color:#ff7b72;font-weight:bold">]</span> and IPs <span style="color:#ff7b72;font-weight:bold">[</span>192.168.1.1 127.0.0.1 ::1<span style="color:#ff7b72;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>certs<span style="color:#ff7b72;font-weight:bold">]</span> Generating <span style="color:#a5d6ff">&#34;etcd/healthcheck-client&#34;</span> certificate and key
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>certs<span style="color:#ff7b72;font-weight:bold">]</span> Generating <span style="color:#a5d6ff">&#34;apiserver-etcd-client&#34;</span> certificate and key
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>certs<span style="color:#ff7b72;font-weight:bold">]</span> Generating <span style="color:#a5d6ff">&#34;sa&#34;</span> key and public key
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>kubeconfig<span style="color:#ff7b72;font-weight:bold">]</span> Using kubeconfig folder <span style="color:#a5d6ff">&#34;/etc/kubernetes&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>kubeconfig<span style="color:#ff7b72;font-weight:bold">]</span> Writing <span style="color:#a5d6ff">&#34;admin.conf&#34;</span> kubeconfig file
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>kubeconfig<span style="color:#ff7b72;font-weight:bold">]</span> Writing <span style="color:#a5d6ff">&#34;kubelet.conf&#34;</span> kubeconfig file
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>kubeconfig<span style="color:#ff7b72;font-weight:bold">]</span> Writing <span style="color:#a5d6ff">&#34;controller-manager.conf&#34;</span> kubeconfig file
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>kubeconfig<span style="color:#ff7b72;font-weight:bold">]</span> Writing <span style="color:#a5d6ff">&#34;scheduler.conf&#34;</span> kubeconfig file
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>kubelet-start<span style="color:#ff7b72;font-weight:bold">]</span> Writing kubelet environment file with flags to file <span style="color:#a5d6ff">&#34;/var/lib/kubelet/kubeadm-flags.env&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>kubelet-start<span style="color:#ff7b72;font-weight:bold">]</span> Writing kubelet configuration to file <span style="color:#a5d6ff">&#34;/var/lib/kubelet/config.yaml&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>kubelet-start<span style="color:#ff7b72;font-weight:bold">]</span> Starting the kubelet
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>control-plane<span style="color:#ff7b72;font-weight:bold">]</span> Using manifest folder <span style="color:#a5d6ff">&#34;/etc/kubernetes/manifests&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>control-plane<span style="color:#ff7b72;font-weight:bold">]</span> Creating static Pod manifest <span style="color:#ff7b72">for</span> <span style="color:#a5d6ff">&#34;kube-apiserver&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>control-plane<span style="color:#ff7b72;font-weight:bold">]</span> Creating static Pod manifest <span style="color:#ff7b72">for</span> <span style="color:#a5d6ff">&#34;kube-controller-manager&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>control-plane<span style="color:#ff7b72;font-weight:bold">]</span> Creating static Pod manifest <span style="color:#ff7b72">for</span> <span style="color:#a5d6ff">&#34;kube-scheduler&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>etcd<span style="color:#ff7b72;font-weight:bold">]</span> Creating static Pod manifest <span style="color:#ff7b72">for</span> local etcd in <span style="color:#a5d6ff">&#34;/etc/kubernetes/manifests&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>wait-control-plane<span style="color:#ff7b72;font-weight:bold">]</span> Waiting <span style="color:#ff7b72">for</span> the kubelet to boot up the control plane as static Pods from directory <span style="color:#a5d6ff">&#34;/etc/kubernetes/manifests&#34;</span>. This can take up to 4m0s
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>apiclient<span style="color:#ff7b72;font-weight:bold">]</span> All control plane components are healthy after 26.502830 seconds
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>upload-config<span style="color:#ff7b72;font-weight:bold">]</span> Storing the configuration used in ConfigMap <span style="color:#a5d6ff">&#34;kubeadm-config&#34;</span> in the <span style="color:#a5d6ff">&#34;kube-system&#34;</span> Namespace
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>kubelet<span style="color:#ff7b72;font-weight:bold">]</span> Creating a ConfigMap <span style="color:#a5d6ff">&#34;kubelet-config-1.19&#34;</span> in namespace kube-system with the configuration <span style="color:#ff7b72">for</span> the kubelets in the cluster
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>upload-certs<span style="color:#ff7b72;font-weight:bold">]</span> Skipping phase. Please see --upload-certs
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>mark-control-plane<span style="color:#ff7b72;font-weight:bold">]</span> Marking the node workflow-1 as control-plane by adding the label <span style="color:#a5d6ff">&#34;node-role.kubernetes.io/master=&#39;&#39;&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>mark-control-plane<span style="color:#ff7b72;font-weight:bold">]</span> Marking the node workflow-1 as control-plane by adding the taints <span style="color:#ff7b72;font-weight:bold">[</span>node-role.kubernetes.io/master:NoSchedule<span style="color:#ff7b72;font-weight:bold">]</span>
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>bootstrap-token<span style="color:#ff7b72;font-weight:bold">]</span> Using token: a8svth.m8zu4mdj60m6zjzd
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>bootstrap-token<span style="color:#ff7b72;font-weight:bold">]</span> Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>bootstrap-token<span style="color:#ff7b72;font-weight:bold">]</span> configured RBAC rules to allow Node Bootstrap tokens to get nodes
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>bootstrap-token<span style="color:#ff7b72;font-weight:bold">]</span> configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order <span style="color:#ff7b72">for</span> nodes to get long term certificate credentials
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>bootstrap-token<span style="color:#ff7b72;font-weight:bold">]</span> configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>bootstrap-token<span style="color:#ff7b72;font-weight:bold">]</span> configured RBAC rules to allow certificate rotation <span style="color:#ff7b72">for</span> all node client certificates in the cluster
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>bootstrap-token<span style="color:#ff7b72;font-weight:bold">]</span> Creating the <span style="color:#a5d6ff">&#34;cluster-info&#34;</span> ConfigMap in the <span style="color:#a5d6ff">&#34;kube-public&#34;</span> namespace
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>kubelet-finalize<span style="color:#ff7b72;font-weight:bold">]</span> Updating <span style="color:#a5d6ff">&#34;/etc/kubernetes/kubelet.conf&#34;</span> to point to a rotatable kubelet client certificate and key
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>addons<span style="color:#ff7b72;font-weight:bold">]</span> Applied essential addon: CoreDNS
</span></span><span style="display:flex;"><span><span style="color:#ff7b72;font-weight:bold">[</span>addons<span style="color:#ff7b72;font-weight:bold">]</span> Applied essential addon: kube-proxy
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Your Kubernetes control-plane has initialized successfully!
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>To start using your cluster, you need to run the following as a regular user:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  mkdir -p <span style="color:#79c0ff">$HOME</span>/.kube
</span></span><span style="display:flex;"><span>  sudo cp -i /etc/kubernetes/admin.conf <span style="color:#79c0ff">$HOME</span>/.kube/config
</span></span><span style="display:flex;"><span>  sudo chown <span style="color:#ff7b72">$(</span>id -u<span style="color:#ff7b72">)</span>:<span style="color:#ff7b72">$(</span>id -g<span style="color:#ff7b72">)</span> <span style="color:#79c0ff">$HOME</span>/.kube/config
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>You should now deploy a pod network to the cluster.
</span></span><span style="display:flex;"><span>Run <span style="color:#a5d6ff">&#34;kubectl apply -f [podnetwork].yaml&#34;</span> with one of the options listed at:
</span></span><span style="display:flex;"><span>  https://kubernetes.io/docs/concepts/cluster-administration/addons/
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Then you can join any number of worker nodes by running the following on each as root:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>kubeadm join 192.168.1.1:6443 --token a8svth.m8zu4mdj60m6zjzd <span style="color:#79c0ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#79c0ff"></span>    --discovery-token-ca-cert-hash sha256:a98361e5d879fe14734236285b8ad28d0a4d3d1470bd424194011bee41ee8c9e 
</span></span></code></pre></div><p>在看到如上的命令之后，需要按照它的说明执行这三条命令，其作用是将kubectl所需要的配置文件拉到用户目录下并设置访问权限。（不执行的话kubectl是无法正常工作的）</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>mkdir -p <span style="color:#79c0ff">$HOME</span>/.kube
</span></span><span style="display:flex;"><span>sudo cp -i /etc/kubernetes/admin.conf <span style="color:#79c0ff">$HOME</span>/.kube/config
</span></span><span style="display:flex;"><span>sudo chown <span style="color:#ff7b72">$(</span>id -u<span style="color:#ff7b72">)</span>:<span style="color:#ff7b72">$(</span>id -g<span style="color:#ff7b72">)</span> <span style="color:#79c0ff">$HOME</span>/.kube/config
</span></span></code></pre></div><p>之后，执行<code>kubectl get pods --all-namespaces</code>(或者<code>-n=kube-system</code>)，就可以看到kubernetes的系统节点了。</p>
<pre tabindex="0"><code>NAME                                 READY   STATUS    RESTARTS   AGE
coredns-f9fd979d6-7m7bb              1/1     Running   0          2m26s
coredns-f9fd979d6-9xr4b              1/1     Running   0          2m26s
etcd-workflow-1                      1/1     Running   0          2m30s
kube-apiserver-workflow-1            1/1     Running   0          2m30s
kube-controller-manager-workflow-1   1/1     Running   0          2m30s
kube-proxy-pdpkp                     1/1     Running   0          2m26s
kube-scheduler-workflow-1            1/1     Running   0          2m30s
</code></pre><p>这里的CoreDNS需要在安装完CNI插件之后才会正常运行，Pending是正常情况。这里是Running，我觉得是因为我第一次的时候没有删除干净。</p>
<h3 id="安装flannel">
  安装flannel
  <a class="heading-link" href="#%e5%ae%89%e8%a3%85flannel">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>参考<a href="https://github.com/coreos/flannel/blob/master/Documentation/kubernetes.md"  class="external-link" target="_blank" rel="noopener">flannel官方教程</a>，对于1.7版本以上的kubernetes，可以直接用<code>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</code>。当然，国内网络，懂得都懂，我就基本上没有直连成功过。直接手动弄下来搞也可以。</p>
<p>日志记录</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>stack@workflow-1:~/k8sTest$ kubectl apply -f kube-flannel.yml 
</span></span><span style="display:flex;"><span>podsecuritypolicy.policy/psp.flannel.unprivileged created
</span></span><span style="display:flex;"><span>Warning: rbac.authorization.k8s.io/v1beta1 ClusterRole is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRole
</span></span><span style="display:flex;"><span>clusterrole.rbac.authorization.k8s.io/flannel created
</span></span><span style="display:flex;"><span>Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding
</span></span><span style="display:flex;"><span>clusterrolebinding.rbac.authorization.k8s.io/flannel created
</span></span><span style="display:flex;"><span>serviceaccount/flannel created
</span></span><span style="display:flex;"><span>configmap/kube-flannel-cfg created
</span></span><span style="display:flex;"><span>daemonset.apps/kube-flannel-ds created
</span></span></code></pre></div><p>可能出现的问题：</p>
<ul>
<li>flannel需要镜像<code>quay.io/coreos/flannel:v0.13.0-rc2</code>，之前的时候我的虚拟机是无法直接从<code>quay.io</code>下拉镜像，因此有可能需要曲线救国。</li>
</ul>
<h3 id="连接其他节点">
  连接其他节点
  <a class="heading-link" href="#%e8%bf%9e%e6%8e%a5%e5%85%b6%e4%bb%96%e8%8a%82%e7%82%b9">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>将主节点装完之后就可以安装其他节点了（也可以在之前一起安装）。其他节点的前置条件需要像主节点一样，包括系统配置、软件安装等等。</p>
<p>比较麻烦的一点是从节点也需要安装那一些镜像，如果我没有理解错的话。这就需要很久了……</p>
<p>命令就是之前弹出来的那个</p>
<pre tabindex="0"><code>sudo kubeadm join 192.168.1.1:6443 --token a8svth.m8zu4mdj60m6zjzd \
    --discovery-token-ca-cert-hash sha256:a98361e5d879fe14734236285b8ad28d0a4d3d1470bd424194011bee41ee8c9e 
</code></pre><p>可能出现的问题：</p>
<ul>
<li>如果安装过比较多次的话，会发现一些奇怪的报错信息，比如端口被占用之类的。请记得执行<code>kubeadm reset</code></li>
</ul>
<h2 id="安装istio">
  安装istio
  <a class="heading-link" href="#%e5%ae%89%e8%a3%85istio">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>原本计划是用helm来安装istio，但是最新版本的<a href="https://istio.io/latest/zh/docs/setup/install/helm/"  class="external-link" target="_blank" rel="noopener">官方文档</a>弃用了helm。</p>
<h3 id="下载">
  下载
  <a class="heading-link" href="#%e4%b8%8b%e8%bd%bd">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>首先到官方给的<a href="https://github.com/istio/istio/releases/tag/1.7.3"  class="external-link" target="_blank" rel="noopener">发布页面</a>下载（这个链接是1.7.3版本）。</p>
<p>然后使用<code>tar -zxvf</code>解压</p>
<h3 id="安装">
  安装
  <a class="heading-link" href="#%e5%ae%89%e8%a3%85">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>按照官方说明<a href="https://istio.io/latest/zh/docs/setup/getting-started/#download"  class="external-link" target="_blank" rel="noopener">文档</a></p>
<ul>
<li>把文件夹下的<code>istioctl</code>所在目录加入PATH变量之中。（我没有把istioctl丢到<code>/usr/bin</code>下，因为它这个安装过程有可能要用到其他目录中的文件）</li>
<li>执行<code>istioctl manifest install --set profile=demo</code></li>
</ul>
<pre tabindex="0"><code>stack@workflow-1:~/k8sTest/istio-1.7.3/bin$ istioctl manifest install --set profile=demo
Detected that your cluster does not support third party JWT authentication. Falling back to less secure first party JWT. See https://istio.io/docs/ops/best-practices/security/#configure-third-party-service-account-tokens for details.
✔ Istio core installed                                                               
✔ Istiod installed                                                                   
✔ Egress gateways installed                                                          
✔ Ingress gateways installed                                                         
✔ Installation complete 
</code></pre><p>结果：</p>
<pre tabindex="0"><code>stack@workflow-1:~/k8sTest/istio-1.7.3/bin$ kubectl get pods -n=istio-system
NAME                                    READY   STATUS    RESTARTS   AGE
istio-egressgateway-8556f8c8dc-rj922    1/1     Running   0          2m42s
istio-ingressgateway-589d868684-pf7g4   1/1     Running   0          2m42s
istiod-86d65b6959-lch8x                 1/1     Running   0          3m54s
stack@workflow-1:~/k8sTest/istio-1.7.3/bin$ kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                      AGE
istio-egressgateway    ClusterIP      10.108.56.199   &lt;none&gt;        80/TCP,443/TCP,15443/TCP                                                     3m7s
istio-ingressgateway   LoadBalancer   10.96.27.251    &lt;pending&gt;     15021:32083/TCP,80:31588/TCP,443:31926/TCP,31400:30951/TCP,15443:31996/TCP   3m7s
istiod                 ClusterIP      10.111.3.121    &lt;none&gt;        15010/TCP,15012/TCP,443/TCP,15014/TCP,853/TCP                                4m20s
</code></pre><p>这个步骤真的是比我当时（0.2.x）要简单多了。我记得我试了好多次都没有成功，都有心理阴影了。才过了一年多就有了这么大的变化，软件行业真的是日新月异啊。</p>
<h2 id="部署prometheus和grafana">
  部署prometheus和grafana
  <a class="heading-link" href="#%e9%83%a8%e7%bd%b2prometheus%e5%92%8cgrafana">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>参考资料：</p>
<ul>
<li><a href="https://www.jianshu.com/p/ac8853927528"  class="external-link" target="_blank" rel="noopener">简书-k8s安装prometheus+grafana</a></li>
<li><a href="https://github.com/prometheus/prometheus"  class="external-link" target="_blank" rel="noopener">prometheus-github</a></li>
<li><a href="https://juejin.im/post/6844903908251451406"  class="external-link" target="_blank" rel="noopener">掘金-k8s监控 安装prometheus</a></li>
<li><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/"  class="external-link" target="_blank" rel="noopener">promethues官方-配置文件规范</a></li>
</ul>
<p>istio之前有一套prometheus的系统，因此我一开始并没有考虑安装。但是在<a href="https://istio.io/latest/docs/ops/integrations/prometheus/"  class="external-link" target="_blank" rel="noopener">最新版本1.7</a>中它并没有自带了，只是在addon里面加上了一个prometheus的样本用于验证（不过似乎也够用？）。</p>
<p><a href="https://github.com/istio/istio/tree/master/samples/addons"  class="external-link" target="_blank" rel="noopener">istio/addon</a></p>
<p>可以直接使用<code>kubectl apply -f samples/addons</code>进行安装。当然如果之前是自定义安装istioctl的话那这一步就需要看一下官方文档怎么说了。</p>
<p>可能出现的问题：</p>
<ul>
<li>在K8s 1.19版本情况下可能会出现如<a href="https://github.com/istio/istio/issues/27417"  class="external-link" target="_blank" rel="noopener">issues</a>所示的情况，因为某些未知的原因k8s无法识别出在同一个文件内配置的配置项，因此必须要执行两次<code>kubectl apply -f kiali.yaml</code></li>
</ul>

      </div>


      <footer>
        


        
        
        
        
        

        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2020 -
    
    2024
     Timothy Wu 
    ·
    
     <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js" integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>

</html>
