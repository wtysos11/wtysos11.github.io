<!DOCTYPE html>
<html lang="zh">

<head>
  <title>
  Keras中的Embedding层是如何工作的 · 实践出真知
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Timothy Wu">
<meta name="description" content="在学习的过程中遇到了这个问题，同时也看到了SO中有相同的问题。而keras-github中这个问题也挺有意思的，记录一下。
这个解释很不错，假如现在有这么两句话
Hope to see you soon Nice to see you again 在神经网络中，我们将这个作为输入，一般就会将每个单词用一个正整数代替，这样，上面的两句话在输入中是这样的
[0, 1, 2, 3, 4] [5, 1, 2, 3, 6] 在神经网络中，第一层是
Embedding(7, 2, input_length=5) 其中，第一个参数是input_dim，上面的值是7，代表的是单词表的长度；第二个参数是output_dim，上面的值是2，代表输出后向量长度为2；第三个参数是input_length，上面的值是5，代表输入序列的长度。
一旦神经网络被训练了，Embedding层就会被赋予一个权重，计算出来的结果如下：
&#43;------------&#43;------------&#43; | index | Embedding | &#43;------------&#43;------------&#43; | 0 | [1.2, 3.1] | | 1 | [0.1, 4.2] | | 2 | [1.0, 3.1] | | 3 | [0.3, 2.1] | | 4 | [2.2, 1.4] | | 5 | [0.7, 1.">
<meta name="keywords" content="blog,developer,personal">

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Keras中的Embedding层是如何工作的"/>
<meta name="twitter:description" content="在学习的过程中遇到了这个问题，同时也看到了SO中有相同的问题。而keras-github中这个问题也挺有意思的，记录一下。
这个解释很不错，假如现在有这么两句话
Hope to see you soon Nice to see you again 在神经网络中，我们将这个作为输入，一般就会将每个单词用一个正整数代替，这样，上面的两句话在输入中是这样的
[0, 1, 2, 3, 4] [5, 1, 2, 3, 6] 在神经网络中，第一层是
Embedding(7, 2, input_length=5) 其中，第一个参数是input_dim，上面的值是7，代表的是单词表的长度；第二个参数是output_dim，上面的值是2，代表输出后向量长度为2；第三个参数是input_length，上面的值是5，代表输入序列的长度。
一旦神经网络被训练了，Embedding层就会被赋予一个权重，计算出来的结果如下：
&#43;------------&#43;------------&#43; | index | Embedding | &#43;------------&#43;------------&#43; | 0 | [1.2, 3.1] | | 1 | [0.1, 4.2] | | 2 | [1.0, 3.1] | | 3 | [0.3, 2.1] | | 4 | [2.2, 1.4] | | 5 | [0.7, 1."/>

<meta property="og:title" content="Keras中的Embedding层是如何工作的" />
<meta property="og:description" content="在学习的过程中遇到了这个问题，同时也看到了SO中有相同的问题。而keras-github中这个问题也挺有意思的，记录一下。
这个解释很不错，假如现在有这么两句话
Hope to see you soon Nice to see you again 在神经网络中，我们将这个作为输入，一般就会将每个单词用一个正整数代替，这样，上面的两句话在输入中是这样的
[0, 1, 2, 3, 4] [5, 1, 2, 3, 6] 在神经网络中，第一层是
Embedding(7, 2, input_length=5) 其中，第一个参数是input_dim，上面的值是7，代表的是单词表的长度；第二个参数是output_dim，上面的值是2，代表输出后向量长度为2；第三个参数是input_length，上面的值是5，代表输入序列的长度。
一旦神经网络被训练了，Embedding层就会被赋予一个权重，计算出来的结果如下：
&#43;------------&#43;------------&#43; | index | Embedding | &#43;------------&#43;------------&#43; | 0 | [1.2, 3.1] | | 1 | [0.1, 4.2] | | 2 | [1.0, 3.1] | | 3 | [0.3, 2.1] | | 4 | [2.2, 1.4] | | 5 | [0.7, 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://wtysos11.github.io/posts/20200908_keras%E4%B8%AD%E7%9A%84embedding%E5%B1%82%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-08T20:57:04+08:00" />
<meta property="article:modified_time" content="2024-01-21T00:00:00+00:00" />





<link rel="canonical" href="http://wtysos11.github.io/posts/20200908_keras%E4%B8%AD%E7%9A%84embedding%E5%B1%82%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.577e3c5ead537873430da16f0964b754a120fd87c4e2203a00686e7c75b51378.css" integrity="sha256-V348Xq1TeHNDDaFvCWS3VKEg/YfE4iA6AGhufHW1E3g=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css" integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin="anonymous" media="screen" />
  



 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      实践出真知
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/tags/">Tags</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/categories/">Categories</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/series/">series</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://wtysos11.github.io/posts/20200908_keras%E4%B8%AD%E7%9A%84embedding%E5%B1%82%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84/">
              Keras中的Embedding层是如何工作的
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2020-09-08T20:57:04&#43;08:00">
                九月 8, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              
            </span>
          </div>
          
          <div class="categories">
  <i class="fa-solid fa-folder" aria-hidden="true"></i>
    <a href="/categories/%E9%97%AE%E9%A2%98/">问题</a></div>

          <div class="tags">
  <i class="fa-solid fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/%E5%86%85%E5%AE%B9/%E9%97%AE%E9%A2%98/">内容/问题</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/keras/">计算机/Keras</a>
    </span></div>

        </div>
      </header>

      <div class="post-content">
        
        <p>在学习的过程中遇到了这个问题，同时也看到了<a href="https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work"  class="external-link" target="_blank" rel="noopener">SO</a>中有相同的问题。而keras-<a href="https://github.com/keras-team/keras/issues/3110"  class="external-link" target="_blank" rel="noopener">github</a>中这个问题也挺有意思的，记录一下。</p>
<p>这个解释很不错，假如现在有这么两句话</p>
<ul>
<li>Hope to see you soon</li>
<li>Nice to see you again</li>
</ul>
<p>在神经网络中，我们将这个作为输入，一般就会将每个单词用一个正整数代替，这样，上面的两句话在输入中是这样的</p>
<pre tabindex="0"><code>[0, 1, 2, 3, 4]

[5, 1, 2, 3, 6]
</code></pre><p>在神经网络中，第一层是</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Embedding(<span style="color:#a5d6ff">7</span>, <span style="color:#a5d6ff">2</span>, input_length<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">5</span>)
</span></span></code></pre></div><p>其中，第一个参数是input_dim，上面的值是7，代表的是单词表的长度；第二个参数是output_dim，上面的值是2，代表输出后向量长度为2；第三个参数是input_length，上面的值是5，代表输入序列的长度。</p>
<p>一旦神经网络被训练了，Embedding层就会被赋予一个权重，计算出来的结果如下：</p>
<pre tabindex="0"><code>+------------+------------+
|   index    |  Embedding |
+------------+------------+
|     0      | [1.2, 3.1] |
|     1      | [0.1, 4.2] |
|     2      | [1.0, 3.1] |
|     3      | [0.3, 2.1] |
|     4      | [2.2, 1.4] |
|     5      | [0.7, 1.7] |
|     6      | [4.1, 2.0] |
+------------+------------+
</code></pre><p>根据这个权重，第二个输入计算出来的embedding vector就是下面这个：</p>
<pre tabindex="0"><code>[[0.7, 1.7], [0.1, 4.2], [1.0, 3.1], [0.3, 2.1], [4.1, 2.0]]
</code></pre><p>原理上，从keras的那个<a href="https://github.com/keras-team/keras/issues/3110"  class="external-link" target="_blank" rel="noopener">issue</a>可以看到，在执行过程中实际上是查表，将输入的整数作为index，去检索矩阵的对应行，并将值取出。至于这个embedding matrix是怎么维护的我还没有搞明白。</p>

      </div>


      <footer>
        


        
        
        
        
        

        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2020 -
    
    2024
     Timothy Wu 
    ·
    
     <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js" integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>

</html>
